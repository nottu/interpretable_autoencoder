{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage.interpolation import rotate as sc_rotate\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.optimize import curve_fit, least_squares, minimize\n",
    "\n",
    "from pandas import read_fwf, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radioreader import *\n",
    "from methods import *\n",
    "from kittler import kittler_float\n",
    "\n",
    "from InterpretableAE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'unlrg'\n",
    "ext = 'fits'\n",
    "names = glob.glob('{0}/*.{1}*'.format(directory, ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1d9ea4191c4baabdaed2f2204b2ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14245), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "for n in tqdm(range(len(names))):\n",
    "    im = readImg(names[n], normalize=True, sz=128)\n",
    "    k = kittler_float(im, copy=False)\n",
    "    images.append( np.expand_dims(k, axis=0) )\n",
    "    del im\n",
    "    del k\n",
    "# images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14245, 1, 128, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_tensor = torch.tensor(images)\n",
    "im_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([14245, 1, 128, 128])\n",
      "imh, imw =  128 128\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#arguments\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "ts = list(im_tensor.shape)\n",
    "imh, imw = ts[2], ts[3]\n",
    "print(im_tensor.shape)\n",
    "print('imh, imw = ',imh, imw)\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9 # 'SGD momentum'\n",
    "latent_space=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InterpretableAE(imh, imw, device, latent_dim=latent_space).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_loader = torch.utils.data.DataLoader(im_tensor, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=5):\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        # Reshape data\n",
    "        rot_data = random_rotate(data.numpy())\n",
    "        targets, angles = rotate_tensor(rot_data)\n",
    "        targets = torch.from_numpy(targets).to(device, dtype=torch.float)\n",
    "        angles = torch.from_numpy(angles).to(device)\n",
    "        angles = angles.view(angles.size(0), 1)\n",
    "\n",
    "        # Forward pass\n",
    "        rot_data = torch.from_numpy(rot_data).to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(rot_data, angles)\n",
    "\n",
    "        # Binary cross entropy loss\n",
    "        loss_fnc = nn.BCELoss(reduction='sum')\n",
    "        loss = loss_fnc(output, targets)\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            sys.stdout.write('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\r'\n",
    "                .format(epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14080/14245 (99%)]\tLoss: 18783.781250\n",
      " Time: 58.45s\n",
      "Train Epoch: 2 [14080/14245 (99%)]\tLoss: 11834.796875\n",
      " Time: 56.95s\n",
      "Train Epoch: 3 [14080/14245 (99%)]\tLoss: 8394.5722660\n",
      " Time: 57.19s\n",
      "Train Epoch: 4 [14080/14245 (99%)]\tLoss: 12283.357422\n",
      " Time: 56.44s\n",
      "Train Epoch: 5 [14080/14245 (99%)]\tLoss: 10662.165039\n",
      " Time: 56.40s\n",
      "Train Epoch: 6 [14080/14245 (99%)]\tLoss: 8907.3261727\n",
      " Time: 56.32s\n",
      "Train Epoch: 7 [14080/14245 (99%)]\tLoss: 9032.2421884\n",
      " Time: 56.62s\n",
      "Train Epoch: 8 [14080/14245 (99%)]\tLoss: 11177.343750\n",
      " Time: 56.69s\n",
      "Train Epoch: 9 [14080/14245 (99%)]\tLoss: 10382.126953\n",
      " Time: 56.90s\n",
      "Train Epoch: 10 [14080/14245 (99%)]\tLoss: 10356.609375\n",
      " Time: 56.48s\n",
      "Train Epoch: 11 [14080/14245 (99%)]\tLoss: 9618.6142586\n",
      " Time: 57.06s\n",
      "Train Epoch: 12 [14080/14245 (99%)]\tLoss: 8237.0878915\n",
      " Time: 56.77s\n",
      "Train Epoch: 13 [14080/14245 (99%)]\tLoss: 7476.0268553\n",
      " Time: 57.22s\n",
      "Train Epoch: 14 [14080/14245 (99%)]\tLoss: 7170.4946297\n",
      " Time: 56.32s\n",
      "Train Epoch: 15 [14080/14245 (99%)]\tLoss: 8938.368164\n",
      " Time: 56.21s\n",
      "Train Epoch: 16 [14080/14245 (99%)]\tLoss: 7072.708984\n",
      " Time: 56.71s\n",
      "Train Epoch: 17 [14080/14245 (99%)]\tLoss: 7862.166016\n",
      " Time: 56.74s\n",
      "Train Epoch: 18 [14080/14245 (99%)]\tLoss: 9861.224609\n",
      " Time: 56.41s\n",
      "Train Epoch: 19 [14080/14245 (99%)]\tLoss: 9139.2753912\n",
      " Time: 56.28s\n",
      "Train Epoch: 20 [14080/14245 (99%)]\tLoss: 10645.188477\n",
      " Time: 56.24s\n",
      "Train Epoch: 21 [14080/14245 (99%)]\tLoss: 7969.964355\n",
      " Time: 56.24s\n",
      "Train Epoch: 22 [14080/14245 (99%)]\tLoss: 18572.703125\n",
      " Time: 56.19s\n",
      "Train Epoch: 23 [14080/14245 (99%)]\tLoss: 10938.195312\n",
      " Time: 56.09s\n",
      "Train Epoch: 24 [14080/14245 (99%)]\tLoss: 7161.5136724\n",
      " Time: 56.23s\n",
      "Train Epoch: 25 [14080/14245 (99%)]\tLoss: 7689.393555\n",
      " Time: 56.16s\n",
      "Train Epoch: 26 [14080/14245 (99%)]\tLoss: 17528.183594\n",
      " Time: 56.47s\n",
      "Train Epoch: 27 [14080/14245 (99%)]\tLoss: 6754.890625\n",
      " Time: 56.18s\n",
      "Train Epoch: 28 [14080/14245 (99%)]\tLoss: 8339.0449228\n",
      " Time: 56.14s\n",
      "Train Epoch: 29 [14080/14245 (99%)]\tLoss: 8658.6621094\n",
      " Time: 56.25s\n",
      "Train Epoch: 30 [14080/14245 (99%)]\tLoss: 7322.152344\n",
      " Time: 56.19s\n",
      "Train Epoch: 31 [14080/14245 (99%)]\tLoss: 8764.5068368\n",
      " Time: 56.08s\n",
      "Train Epoch: 32 [14080/14245 (99%)]\tLoss: 7511.8129884\n",
      " Time: 56.08s\n",
      "Train Epoch: 33 [14080/14245 (99%)]\tLoss: 6591.213867\n",
      " Time: 56.15s\n",
      "Train Epoch: 34 [14080/14245 (99%)]\tLoss: 9396.142578\n",
      " Time: 56.17s\n",
      "Train Epoch: 35 [14080/14245 (99%)]\tLoss: 7402.790527\n",
      " Time: 56.23s\n",
      "Train Epoch: 36 [14080/14245 (99%)]\tLoss: 6944.714844\n",
      " Time: 56.25s\n",
      "Train Epoch: 37 [14080/14245 (99%)]\tLoss: 6583.960938\n",
      " Time: 56.20s\n",
      "Train Epoch: 38 [14080/14245 (99%)]\tLoss: 7337.563477\n",
      " Time: 56.09s\n",
      "Train Epoch: 39 [14080/14245 (99%)]\tLoss: 7958.515625\n",
      " Time: 56.21s\n",
      "Train Epoch: 40 [14080/14245 (99%)]\tLoss: 7611.001465\n",
      " Time: 56.26s\n",
      "Train Epoch: 41 [14080/14245 (99%)]\tLoss: 6801.731934\n",
      " Time: 57.21s\n",
      "Train Epoch: 42 [14080/14245 (99%)]\tLoss: 7314.178711\n",
      " Time: 57.07s\n",
      "Train Epoch: 43 [14080/14245 (99%)]\tLoss: 8050.659180\n",
      " Time: 56.81s\n",
      "Train Epoch: 44 [14080/14245 (99%)]\tLoss: 7156.274902\n",
      " Time: 56.23s\n",
      "Train Epoch: 45 [14080/14245 (99%)]\tLoss: 7155.882812\n",
      " Time: 56.13s\n",
      "Train Epoch: 46 [14080/14245 (99%)]\tLoss: 7997.724121\n",
      " Time: 56.17s\n",
      "Train Epoch: 47 [14080/14245 (99%)]\tLoss: 5291.790527\n",
      " Time: 56.01s\n",
      "Train Epoch: 48 [14080/14245 (99%)]\tLoss: 6468.183594\n",
      " Time: 56.21s\n",
      "Train Epoch: 49 [14080/14245 (99%)]\tLoss: 9861.366211\n",
      " Time: 56.25s\n",
      "Train Epoch: 50 [14080/14245 (99%)]\tLoss: 6995.1474610\n",
      " Time: 56.24s\n",
      "Train Epoch: 51 [14080/14245 (99%)]\tLoss: 6627.952148\n",
      " Time: 56.24s\n",
      "Train Epoch: 52 [14080/14245 (99%)]\tLoss: 6997.185547\n",
      " Time: 56.21s\n",
      "Train Epoch: 53 [14080/14245 (99%)]\tLoss: 8392.883789\n",
      " Time: 56.20s\n",
      "Train Epoch: 54 [14080/14245 (99%)]\tLoss: 6888.3632814\n",
      " Time: 56.26s\n",
      "Train Epoch: 55 [14080/14245 (99%)]\tLoss: 6394.738770\n",
      " Time: 56.23s\n",
      "Train Epoch: 56 [14080/14245 (99%)]\tLoss: 6917.253906\n",
      " Time: 56.23s\n",
      "Train Epoch: 57 [14080/14245 (99%)]\tLoss: 8991.140625\n",
      " Time: 56.24s\n",
      "Train Epoch: 58 [14080/14245 (99%)]\tLoss: 6984.481934\n",
      " Time: 56.19s\n",
      "Train Epoch: 59 [14080/14245 (99%)]\tLoss: 8289.375000\n",
      " Time: 56.12s\n",
      "Train Epoch: 60 [14080/14245 (99%)]\tLoss: 7095.870117\n",
      " Time: 56.15s\n",
      "Train Epoch: 61 [14080/14245 (99%)]\tLoss: 7245.833008\n",
      " Time: 56.28s\n",
      "Train Epoch: 62 [14080/14245 (99%)]\tLoss: 6524.114258\n",
      " Time: 56.19s\n",
      "Train Epoch: 63 [14080/14245 (99%)]\tLoss: 5803.7534185\n",
      " Time: 56.21s\n",
      "Train Epoch: 64 [14080/14245 (99%)]\tLoss: 8505.463867\n",
      " Time: 56.23s\n",
      "Train Epoch: 65 [14080/14245 (99%)]\tLoss: 7177.236328\n",
      " Time: 56.10s\n",
      "Train Epoch: 66 [14080/14245 (99%)]\tLoss: 5946.747070\n",
      " Time: 56.16s\n",
      "Train Epoch: 67 [14080/14245 (99%)]\tLoss: 8012.800293\n",
      " Time: 56.18s\n",
      "Train Epoch: 68 [14080/14245 (99%)]\tLoss: 7541.123047\n",
      " Time: 56.25s\n",
      "Train Epoch: 69 [14080/14245 (99%)]\tLoss: 6239.295898\n",
      " Time: 56.23s\n",
      "Train Epoch: 70 [14080/14245 (99%)]\tLoss: 7705.977051\n",
      " Time: 56.18s\n",
      "Train Epoch: 71 [14080/14245 (99%)]\tLoss: 7293.026367\n",
      " Time: 56.37s\n",
      "Train Epoch: 72 [14080/14245 (99%)]\tLoss: 9014.309570\n",
      " Time: 56.26s\n",
      "Train Epoch: 73 [14080/14245 (99%)]\tLoss: 7799.831055\n",
      " Time: 56.51s\n",
      "Train Epoch: 74 [14080/14245 (99%)]\tLoss: 6538.392090\n",
      " Time: 56.26s\n",
      "Train Epoch: 75 [14080/14245 (99%)]\tLoss: 6659.875000\n",
      " Time: 56.24s\n",
      "Train Epoch: 76 [14080/14245 (99%)]\tLoss: 6481.866211\n",
      " Time: 56.28s\n",
      "Train Epoch: 77 [14080/14245 (99%)]\tLoss: 6595.111328\n",
      " Time: 56.02s\n",
      "Train Epoch: 78 [14080/14245 (99%)]\tLoss: 6110.0366214\n",
      " Time: 56.09s\n",
      "Train Epoch: 79 [14080/14245 (99%)]\tLoss: 7582.554688\n",
      " Time: 55.99s\n",
      "Train Epoch: 80 [14080/14245 (99%)]\tLoss: 5725.561035\n",
      " Time: 56.10s\n",
      "Train Epoch: 81 [14080/14245 (99%)]\tLoss: 5564.107910\n",
      " Time: 56.16s\n",
      "Train Epoch: 82 [14080/14245 (99%)]\tLoss: 4924.974609\n",
      " Time: 56.02s\n",
      "Train Epoch: 83 [14080/14245 (99%)]\tLoss: 6157.493652\n",
      " Time: 55.99s\n",
      "Train Epoch: 84 [14080/14245 (99%)]\tLoss: 7042.331543\n",
      " Time: 56.01s\n",
      "Train Epoch: 85 [14080/14245 (99%)]\tLoss: 7435.991211\n",
      " Time: 56.16s\n",
      "Train Epoch: 86 [14080/14245 (99%)]\tLoss: 8345.061523\n",
      " Time: 56.31s\n",
      "Train Epoch: 87 [14080/14245 (99%)]\tLoss: 6292.948730\n",
      " Time: 56.24s\n",
      "Train Epoch: 88 [14080/14245 (99%)]\tLoss: 7868.723145\n",
      " Time: 56.24s\n",
      "Train Epoch: 89 [14080/14245 (99%)]\tLoss: 6555.278809\n",
      " Time: 56.18s\n",
      "Train Epoch: 90 [14080/14245 (99%)]\tLoss: 7223.837891\n",
      " Time: 56.24s\n",
      "Train Epoch: 91 [14080/14245 (99%)]\tLoss: 8467.113281\n",
      " Time: 56.26s\n",
      "Train Epoch: 92 [14080/14245 (99%)]\tLoss: 5869.095703\n",
      " Time: 56.22s\n",
      "Train Epoch: 93 [14080/14245 (99%)]\tLoss: 6471.146484\n",
      " Time: 56.29s\n",
      "Train Epoch: 94 [14080/14245 (99%)]\tLoss: 7130.002930\n",
      " Time: 56.26s\n",
      "Train Epoch: 95 [14080/14245 (99%)]\tLoss: 6946.869141\n",
      " Time: 56.24s\n",
      "Train Epoch: 96 [14080/14245 (99%)]\tLoss: 6603.739258\n",
      " Time: 56.22s\n",
      "Train Epoch: 97 [14080/14245 (99%)]\tLoss: 5317.2817381\n",
      " Time: 56.31s\n",
      "Train Epoch: 98 [14080/14245 (99%)]\tLoss: 6113.318359\n",
      " Time: 56.18s\n",
      "Train Epoch: 99 [7040/14245 (49%)]\tLoss: 6602.765625\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start = time.time()\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    end = time.time()\n",
    "    sys.stdout.write('\\n Time: {0:.2f}s\\n'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'unlrg_conv_model_latent16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
